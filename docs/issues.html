<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Bayesian Basics</title>
  <meta name="description" content="This document provides an introduction to Bayesian data analysis. It is conceptual in nature, but uses the probabilistic programming language Stan for demonstration (and its implementation in R via rstan). From elementary examples, guidance is provided for data preparation, efficient modeling, diagnostics, and more.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Bayesian Basics" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://m-clark.github.io/bayesian-basics/" />
  <meta property="og:image" content="https://m-clark.github.io/bayesian-basics/img/nineteeneightyR.png" />
  <meta property="og:description" content="This document provides an introduction to Bayesian data analysis. It is conceptual in nature, but uses the probabilistic programming language Stan for demonstration (and its implementation in R via rstan). From elementary examples, guidance is provided for data preparation, efficient modeling, diagnostics, and more." />
  <meta name="github-repo" content="m-clark/bayesian-basics/" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Bayesian Basics" />
  
  <meta name="twitter:description" content="This document provides an introduction to Bayesian data analysis. It is conceptual in nature, but uses the probabilistic programming language Stan for demonstration (and its implementation in R via rstan). From elementary examples, guidance is provided for data preparation, efficient modeling, diagnostics, and more." />
  <meta name="twitter:image" content="https://m-clark.github.io/bayesian-basics/img/nineteeneightyR.png" />



<meta name="date" content="2018-05-17">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="model-enhancements.html">
<link rel="next" href="final-thoughts.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.0/htmlwidgets.js"></script>
<script src="libs/datatables-binding-0.2/datatables.js"></script>
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.12/js/jquery.dataTables.min.js"></script>
<script src="libs/plotly-binding-4.7.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotlyjs-1.29.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotlyjs-1.29.2/plotly-latest.min.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="css/book.css" type="text/css" />
<link rel="stylesheet" href="css/standard_html.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><img src="img/r_and_stan.png" style="width:75%; padding:10px 10px;"></a></li> 

<li class="divider"></li>
<li><a href="index.html#section"></a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#note"><i class="fa fa-check"></i>Note</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a><ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#bayesian-probability"><i class="fa fa-check"></i>Bayesian Probability</a><ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#conditional-probability-bayes-theorem"><i class="fa fa-check"></i>Conditional probability &amp; Bayes theorem</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="a-hands-on-example.html"><a href="a-hands-on-example.html"><i class="fa fa-check"></i>A Hands-on Example</a><ul>
<li class="chapter" data-level="" data-path="a-hands-on-example.html"><a href="a-hands-on-example.html#prior-likelihood-posterior-distributions"><i class="fa fa-check"></i>Prior, likelihood, &amp; posterior distributions</a></li>
<li class="chapter" data-level="" data-path="a-hands-on-example.html"><a href="a-hands-on-example.html#prior"><i class="fa fa-check"></i>Prior</a></li>
<li class="chapter" data-level="" data-path="a-hands-on-example.html"><a href="a-hands-on-example.html#likelihood"><i class="fa fa-check"></i>Likelihood</a></li>
<li class="chapter" data-level="" data-path="a-hands-on-example.html"><a href="a-hands-on-example.html#posterior"><i class="fa fa-check"></i>Posterior</a></li>
<li class="chapter" data-level="" data-path="a-hands-on-example.html"><a href="a-hands-on-example.html#posterior-predictive"><i class="fa fa-check"></i>Posterior predictive</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="regression-models.html"><a href="regression-models.html"><i class="fa fa-check"></i>Regression Models</a><ul>
<li class="chapter" data-level="" data-path="regression-models.html"><a href="regression-models.html#example-linear-regression-model"><i class="fa fa-check"></i>Example: Linear Regression Model</a><ul>
<li class="chapter" data-level="" data-path="regression-models.html"><a href="regression-models.html#setup"><i class="fa fa-check"></i>Setup</a></li>
<li class="chapter" data-level="" data-path="regression-models.html"><a href="regression-models.html#stan-code"><i class="fa fa-check"></i>Stan Code</a></li>
<li class="chapter" data-level="" data-path="regression-models.html"><a href="regression-models.html#running-the-model"><i class="fa fa-check"></i>Running the Model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="model-exploration.html"><a href="model-exploration.html"><i class="fa fa-check"></i>Model Exploration</a><ul>
<li class="chapter" data-level="" data-path="model-exploration.html"><a href="model-exploration.html#monitoring-convergence"><i class="fa fa-check"></i>Monitoring Convergence</a><ul>
<li class="chapter" data-level="" data-path="model-exploration.html"><a href="model-exploration.html#visual-inspection-traceplot-densities"><i class="fa fa-check"></i>Visual Inspection: Traceplot &amp; Densities</a></li>
<li class="chapter" data-level="" data-path="model-exploration.html"><a href="model-exploration.html#statistical-measures"><i class="fa fa-check"></i>Statistical Measures</a></li>
<li class="chapter" data-level="" data-path="model-exploration.html"><a href="model-exploration.html#autocorrelation"><i class="fa fa-check"></i>Autocorrelation</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="model-exploration.html"><a href="model-exploration.html#model-checking"><i class="fa fa-check"></i>Model Checking</a><ul>
<li class="chapter" data-level="" data-path="model-exploration.html"><a href="model-exploration.html#sensitivity-analysis"><i class="fa fa-check"></i>Sensitivity Analysis</a></li>
<li class="chapter" data-level="" data-path="model-exploration.html"><a href="model-exploration.html#predictive-accuracy-model-comparison"><i class="fa fa-check"></i>Predictive Accuracy &amp; Model Comparison</a></li>
<li class="chapter" data-level="" data-path="model-exploration.html"><a href="model-exploration.html#posterior-predictive-checking-statistical"><i class="fa fa-check"></i>Posterior Predictive Checking: Statistical</a></li>
<li class="chapter" data-level="" data-path="model-exploration.html"><a href="model-exploration.html#posterior-predictive-checking-graphical"><i class="fa fa-check"></i>Posterior Predictive Checking: Graphical</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="model-exploration.html"><a href="model-exploration.html#summary"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="model-enhancements.html"><a href="model-enhancements.html"><i class="fa fa-check"></i>Model Enhancements</a><ul>
<li class="chapter" data-level="" data-path="model-enhancements.html"><a href="model-enhancements.html#generating-new-variables-of-interest"><i class="fa fa-check"></i>Generating New Variables of Interest</a></li>
<li class="chapter" data-level="" data-path="model-enhancements.html"><a href="model-enhancements.html#robust-regression"><i class="fa fa-check"></i>Robust Regression</a></li>
<li class="chapter" data-level="" data-path="model-enhancements.html"><a href="model-enhancements.html#generalized-linear-model"><i class="fa fa-check"></i>Generalized Linear Model</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="issues.html"><a href="issues.html"><i class="fa fa-check"></i>Issues</a><ul>
<li class="chapter" data-level="" data-path="issues.html"><a href="issues.html#debugging"><i class="fa fa-check"></i>Debugging</a></li>
<li class="chapter" data-level="" data-path="issues.html"><a href="issues.html#choice-of-prior"><i class="fa fa-check"></i>Choice of Prior</a><ul>
<li class="chapter" data-level="" data-path="issues.html"><a href="issues.html#noninformative-weakly-informative-informative"><i class="fa fa-check"></i>Noninformative, Weakly Informative, Informative</a></li>
<li class="chapter" data-level="" data-path="issues.html"><a href="issues.html#conjugacy"><i class="fa fa-check"></i>Conjugacy</a></li>
<li class="chapter" data-level="" data-path="issues.html"><a href="issues.html#hierarchical-priors"><i class="fa fa-check"></i>Hierarchical Priors</a></li>
<li class="chapter" data-level="" data-path="issues.html"><a href="issues.html#sensitivity-analysis-revisited"><i class="fa fa-check"></i>Sensitivity Analysis Revisited</a></li>
<li class="chapter" data-level="" data-path="issues.html"><a href="issues.html#summary-1"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="issues.html"><a href="issues.html#sampling-procedure"><i class="fa fa-check"></i>Sampling Procedure</a><ul>
<li class="chapter" data-level="" data-path="issues.html"><a href="issues.html#metropolis"><i class="fa fa-check"></i>Metropolis</a></li>
<li class="chapter" data-level="" data-path="issues.html"><a href="issues.html#gibbs"><i class="fa fa-check"></i>Gibbs</a></li>
<li class="chapter" data-level="" data-path="issues.html"><a href="issues.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i>Hamiltonian Monte Carlo</a></li>
<li class="chapter" data-level="" data-path="issues.html"><a href="issues.html#other-variations-and-approximate-methods"><i class="fa fa-check"></i>Other Variations and Approximate Methods</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="issues.html"><a href="issues.html#number-of-draws-thinning-warm-up"><i class="fa fa-check"></i>Number of draws, thinning, warm-up</a></li>
<li class="chapter" data-level="" data-path="issues.html"><a href="issues.html#model-complexity"><i class="fa fa-check"></i>Model Complexity</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="final-thoughts.html"><a href="final-thoughts.html"><i class="fa fa-check"></i>Final Thoughts</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a><ul>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#maximum-likelihood-review"><i class="fa fa-check"></i>Maximum Likelihood Review</a><ul>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#example"><i class="fa fa-check"></i>Example</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#linear-model"><i class="fa fa-check"></i>Linear Model</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#binomial-likelihood-example"><i class="fa fa-check"></i>Binomial Likelihood Example</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#modeling-languages"><i class="fa fa-check"></i>Modeling Languages</a><ul>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#bugs"><i class="fa fa-check"></i>Bugs</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#jags"><i class="fa fa-check"></i>JAGS</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#stan"><i class="fa fa-check"></i>Stan</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#r"><i class="fa fa-check"></i>R</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#general-statistical-packages"><i class="fa fa-check"></i>General Statistical Packages</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#other-programming-languages"><i class="fa fa-check"></i>Other Programming Languages</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#summary-2"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#bugs-example"><i class="fa fa-check"></i>BUGS Example</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#jags-example"><i class="fa fa-check"></i>JAGS Example</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#metropolis-hastings-example"><i class="fa fa-check"></i>Metropolis Hastings Example</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#hamiltonian-monte-carlo-example"><i class="fa fa-check"></i>Hamiltonian Monte Carlo Example</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a><ul>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html#texts-for-your-shelf"><i class="fa fa-check"></i>Texts for Your Shelf</a><ul>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html#stan-specific-resources"><i class="fa fa-check"></i>Stan Specific Resources</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html#other"><i class="fa fa-check"></i>Other</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html#works-citedused"><i class="fa fa-check"></i>Works Cited/Used</a></li>
</ul></li>
<li class="divider"></li>
<li class='after'><a href="https://m-clark.github.io/"><img src="img/mc.png" style="width:50%; padding:0px 0; display:block; margin: 0 auto;" alt="MC logo"></a></li>
<li class='after'><a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="width:50%; border-width:0; display:block; margin: 0 auto;" src="img/ccbysa.png" /></a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><span style="font-size:250%; font-style:italic; font-family:&#39;Alex Brush&#39;">Bayesian Basics</span></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="issues" class="section level1">
<h1>Issues</h1>
<p>This section highlights some things to think about, as well as questions that would naturally arise for the applied researcher who might now be ready to start in on their first Bayesian analysis. It provides merely a taste regarding some select issues, and at this point one should be consulting Bayesian analysis texts directly.</p>
<div id="debugging" class="section level2">
<h2>Debugging</h2>
<p>An essential part of Bayesian analysis is debugging to see if your code and model are doing what it should be doing<a href="#fn47" class="footnoteRef" id="fnref47"><sup>47</sup></a>, and this especially holds for more complex models. For many models with typical numbers for posterior draws, Bayesian analysis might take several minutes on standard computers or laptops. With big data and/or very complex models, some might take hours or even <em>days</em>. In either case, it is a waste of time to let broken code/models run unnecessarily.</p>
<p>The idea with debugging is that, once you think you have everything set up the way you like, run very short attempts to see if A, the code even compiles, and B, whether it runs appropriately. As such, you will only want to set your warm-up and iterations to some small number to begin with, e.g. maybe not even 100 iterations, and no more than two chains<a href="#fn48" class="footnoteRef" id="fnref48"><sup>48</sup></a>. Sometimes it will be obvious what a problem is, such as a typo resulting in the program of choice not being able to locate the parameter of interest. Other issues may be fairly subtle, for example, when it comes to prior specification.</p>
<p>Along with initial short runs, one should consider simpler models first, and perhaps using only a subset of the data. Especially for complex models, it helps to build the model up, debugging and checking for problems along the way. As a not too complicated example, consider a mixed model for logistic regression. One could even start with a standard linear model ignoring the binary nature of the target and random effect structure. Getting a sense of things from that and just making sure that inputs etc. are in place, one can supply the inverse logit link and change the sampling distribution to Bernoulli. Now you can think about adding the random effect, other explanatory variables of interest, and any other complexities that had not been included yet.</p>
<p>As you identify issues, you fix any problems that arise and tinker with other settings. Once you are satisfied, <em>then</em> try for the big run. Even then, you might spot new issues with a longer chain, so you can rinse and repeat at that point. BUGS, JAGS, and Stan more or less have this capacity built in with model upgrade functions. For example, in Stan you can feed the previous setup of a model in to the main <span class="func">stan</span> function. Use one for your initial runs, then when you’re ready, supply the model object as input to the ‘fit’ argument, perhaps with adjustments to the Monte Carlo settings.</p>
</div>
<div id="choice-of-prior" class="section level2">
<h2>Choice of Prior</h2>
<p>Selection of prior distributions might be a bit daunting for the new user of applied Bayesian analysis, but in many cases, and especially for standard models, there are more or less widely adopted choices. Even so, we will discuss the options from a general point of view.</p>
<div id="noninformative-weakly-informative-informative" class="section level3">
<h3>Noninformative, Weakly Informative, Informative</h3>
<p>We can begin with <span class="emph">noninformative priors</span>, which might also be referred to as <em>vague</em>, <em>flat</em>, <em>reference</em>, <em>objective</em>, or <em>diffuse</em> depending on the context. The idea is to use something that allows for Bayesian inference but puts all the premium on the data, and/or soi-disant <em>objectivity</em>, though the fact that there is still choice here should make clear that these are not entirely noninformative or objective. As we have alluded to elsewhere, if we put a prior uniform distribution on the regression coefficients (and e.g. the log of <span class="math inline">\(\sigma\)</span>), this would be a noninformative approach that would essentially be akin to maximum likelihood estimation. One might wonder at this point why we wouldn’t just use vague priors all the time and not worry about overly influencing the analysis by the choice of prior.</p>
<p>As an example, let’s assume a uniform distribution <span class="math inline">\((-\infty,\infty)\)</span> for some parameter <span class="math inline">\(\theta\)</span>. Without bounds, this prior is <em>improper</em>, i.e. the probability distribution does not integrate to 1. While the posterior distribution may be proper, it may not be, and it is left the researcher to determine this. One also has to choose a suitable range, something which may not be easy to ascertain. In addition, the distribution may not be uniform on some transformation of the parameter, say <span class="math inline">\(\theta^2\)</span>. A <em>Jeffreys’ prior</em> could be used to overcome this particular issue, but is more difficult for multiparameter settings.</p>
<p>In general, there are several issues with using a noninformative or reference prior. For many models, there may be no clear choice of what to use. In any case, if the data are sufficient, the prior won’t matter as much, so establishing some reference to be used automatically isn’t exactly in keeping with Bayesian thinking. In addition, such choices can still have unintended effects on the results. Furthermore, if you had clear prior information, e.g. from previous research, one should use it.</p>
<p>In practice, many priors we might use could be said to be <span class="emph">weakly informative</span>. So instead of being completely ignorant, we can choose instead to be mostly ignorant, vague but not too vague. As an example, consider our earlier <a href="a-hands-on-example.html#prior">binomial distribution example</a>. Perhaps a reasonable guess as to the probability of making a penalty kick was .75. With that as a basis, we could choose a Beta distribution that would have roughly 80% of its probability between .6 and .9. We know that lower values for the parameters of a beta distribution represent a less informed state of mind, and the mean of the distribution is A/(A+B), so we could just fiddle with some values to see what we can turn up. The following code suggests a <span class="math inline">\(\mathcal{B}(9,3)\)</span> would probably be a good way to proceed. One can examine such a distribution in the subsequent density plot.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">diff</span>(<span class="kw">pbeta</span>(<span class="kw">c</span>(.<span class="dv">6</span>, .<span class="dv">9</span>), <span class="dv">3</span>, <span class="dv">1</span>))
<span class="kw">diff</span>(<span class="kw">pbeta</span>(<span class="kw">c</span>(.<span class="dv">6</span>, .<span class="dv">9</span>), <span class="dv">8</span>, <span class="dv">3</span>))
<span class="kw">diff</span>(<span class="kw">pbeta</span>(<span class="kw">c</span>(.<span class="dv">6</span>, .<span class="dv">9</span>), <span class="dv">9</span>, <span class="dv">3</span>))</code></pre></div>
<pre><code>[1] 0.513
[1] 0.7625194
[1] 0.7915213</code></pre>
<p><img src="Bayesian-Basics_files/figure-html/chooseBetaPrior2-1.svg" width="500px" style="display: block; margin: auto;" /></p>
<p>With our regression model, we were dealing with standardized predictors, so even choosing a <span class="math inline">\(\mathcal{N}(0, 10)\)</span> might be overly vague, as it would be near flat from -1 to 1, which is the range we’d expect the coefficients to fall in. The nice part about setting the prior mean on zero is that it has a regularizing effect, shrinking coefficients toward 0, that can help avoid overfitting with smaller samples.</p>
<p>Gelman has <a href="https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations#how-informative-is-the-prior">recently proposed</a> the following as a practical check on how informative your prior might be:</p>
<blockquote>
<p>Here’s an idea for not getting tripped up with default priors: For each parameter (or other quantity of interest), compare the posterior sd to the prior sd. If the posterior sd for any parameter is more than 0.1 times the prior sd, then print out a note: “The prior distribution for this parameter is informative.” Then the user can go back and check that the default prior makes sense for this particular example.</p>
</blockquote>
<p>Thus, weakly informative priors can be based on perfectly reasonable settings, and this probably makes more sense than claiming complete ignorance, not to mention they simply work better for parameter estimation relative to flat priors. As mentioned, just some casual thought in many settings will often reveal that one isn’t completely ignorant. Furthermore, if we have clear prior information, in the form of prior research for example, we can then use <span class="emph">informative</span> priors based on those results. This again would be preferable to a completely noninformative approach.</p>
</div>
<div id="conjugacy" class="section level3">
<h3>Conjugacy</h3>
<p>Another consideration in the choice of prior is <span class="emph">conjugacy</span>. Consider using the beta distribution as a prior for the binomial setting as we have done previously. It turns out that using a <span class="math inline">\(\beta(\mathcal{A}, \mathcal{B})\)</span> results in the following posterior:</p>
<p><span class="math display">\[p(\theta|y, n) \propto \beta(y+\mathcal{A}, n-y+\mathcal{B})\]</span></p>
<p>Thus the posterior has the same parametric form as the prior, i.e. the beta distribution is <em>congugate</em> for the binomial likelihood. In this sense, the prior has the interpretation as providing additional data points. In our regression model, the conjugate setting uses a normal distribution for the predictor coefficients and an inverse gamma for <span class="math inline">\(\sigma^2\)</span>. In the case of exponential family distributions of generalized linear models, there are also natural conjugate prior distributions.</p>
<p>While there can be practical advantages to using a conjugate prior, it is not required for some estimation approaches, and for many more complex models, may not even be possible. However, it might be useful to consider a known conjugate prior as a starting point.</p>
</div>
<div id="hierarchical-priors" class="section level3">
<h3>Hierarchical Priors</h3>
<p>Not to be confused with hierarchical linear models, <em>hierarchical</em> in the context of Bayesian models often refers to using what are called <span class="emph">hyperpriors</span>, or priors on priors. Take for instance our example with regression coefficients. Maybe we wouldn’t know a good standard deviation to use. In this case we might set it as yet another parameter to be estimated, <span class="math inline">\(\sigma_\beta\)</span>, and give it a prior with lower bound of 0, e.g. <span class="math inline">\(\sigma_\beta \sim \textrm{Half-Cauchy}(10)\)</span>, that would have a median of 10 but will result in a final estimate of some other value. Technically this could turn into <a href="https://en.wikipedia.org/wiki/Turtles_all_the_way_down#/media/File:River_terrapin.jpg">turtles all the way up</a>, with priors upon priors upon priors. Usually one level is enough though, and might make you feel better for not setting a specific parameter to some value.</p>
</div>
<div id="sensitivity-analysis-revisited" class="section level3">
<h3>Sensitivity Analysis Revisited</h3>
<p>As a reminder, we pointed out in the <a href="model-exploration.html#sensitivity-analysis">sensitivity analysis</a> section of the discussion on model checking, one may perform checks on settings for the model to see if changes to them results in gross changes of inference from the posterior. Part of that check should include the choice of prior, whether different parameter values for the same distribution, or different distributions altogether. Doing such a check will give you more confidence in the final selection.</p>
</div>
<div id="summary-1" class="section level3">
<h3>Summary</h3>
<p>It will not take long with a couple Bayesian texts or research articles that employ Bayesian methods to get a feel for how to go about choosing priors<a href="#fn49" class="footnoteRef" id="fnref49"><sup>49</sup></a>. One should also remember that in the face of a lot of data, the likelihood will overwhelm the prior, rendering the choice effectively moot for simpler models. While the choice might be considered subjective in some respects, it is not arbitrary, and there are standard choices for common models and guidelines for more complex ones to help the researcher in their choice.</p>
</div>
</div>
<div id="sampling-procedure" class="section level2">
<h2>Sampling Procedure</h2>
<p>There are <a href="http://m-clark.github.io/docs/ld_mcmc/">many ways</a> in which one might sample from the posterior. Bayesian analysis is highly flexible and can solve a great many statistical models in theory. In practice things can be more difficult. As more complex models are attempted, new approaches are undertaken to deal with the problems in estimation that inevitably arise. In an attempt to dissolve at least some of the mystery, a brief description follows.</p>
<div id="metropolis" class="section level3">
<h3>Metropolis</h3>
<p>We have mentioned that BUGS and JAGS use <span class="emph">Gibbs sampling</span>, which is a special case of the <span class="emph">Metropolis-Hastings</span> (MH) algorithm<a href="#fn50" class="footnoteRef" id="fnref50"><sup>50</sup></a>, a very general approach encompassing a wide variety of techniques. The Metropolis algorithm can be briefly described in the following steps:</p>
<ol style="list-style-type: decimal">
<li>Start with initial values for the parameters <span class="math inline">\(\theta^0\)</span></li>
</ol>
<p>For <span class="math inline">\(t=1,2...N_{sim}\)</span> :</p>
<ol start="2" style="list-style-type: decimal">
<li>Sample from some proposal distribution a potential candidate <span class="math inline">\(\theta^*\)</span>, given <span class="math inline">\(\theta^{t-1}\)</span></li>
<li>Calculate the ratio <span class="math inline">\(r\)</span> of the posterior densities <span class="math inline">\(\frac{p(\theta^*|y)}{p(\theta^{t-1}|y)}\)</span></li>
<li>Set <span class="math inline">\(\theta^t = \theta^*\)</span> with probability <span class="math inline">\(\min(r, 1)\)</span>, else <span class="math inline">\(\theta^t = \theta^{t-1}\)</span></li>
</ol>
<p>Conceptually, if the proposal increases the posterior density, <span class="math inline">\(\theta^t = \theta^*\)</span>. If it decreases the proposal density, set <span class="math inline">\(\theta^t = \theta^*\)</span> with probability <span class="math inline">\(r\)</span>, else it remains at <span class="math inline">\(\theta^{t-1}\)</span>. The MH algorithm generalizes the Metropolis to use asymmetric proposal distributions and uses an <span class="math inline">\(r\)</span> to correct for asymmetry<a href="#fn51" class="footnoteRef" id="fnref51"><sup>51</sup></a>.</p>
<p>Let’s look at this in generic/pseudo R code for additional clarity (in practice we can take the difference in the log values for step 3):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">nsim =<span class="st"> </span>numberSimulatedDraws
theta0 =<span class="st"> </span>initValue
theta =<span class="st"> </span><span class="kw">c</span>(theta0, <span class="kw">rep</span>(<span class="ot">NA</span>, nsim))

<span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span>nsim) {
  thetaStar =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1</span>, theta[t<span class="op">-</span><span class="dv">1</span>], sd)
  u =<span class="st"> </span><span class="kw">runif</span>(<span class="dv">1</span>)
  r =<span class="st"> </span><span class="kw">exp</span>(logPosterior_thetaStar <span class="op">-</span><span class="st"> </span>logPosterior_theta0)
  theta[t] =<span class="st"> </span><span class="kw">ifelse</span>(u<span class="op">&lt;=</span>r, thetaStar, theta[t<span class="op">-</span><span class="dv">1</span>])
}</code></pre></div>
<p>One can see the <a href="appendix.html#metropolis-hastings-example">Metropolis-Hastings Example</a> to see the Metropolis algorithm applied to our regression problem.</p>
</div>
<div id="gibbs" class="section level3">
<h3>Gibbs</h3>
<p>The Gibbs sampler takes an alternating approach for multiparameter problems, sampling one parameter given the values of the others, and thus reducing a potentially high dimensional problem to lower dimensional conditional densities. We can describe its steps generally as follows.</p>
<p>Start with initial values for some ordering of the parameters <span class="math inline">\(\theta_1^0, \theta_2^0,..., \theta_p^0\)</span></p>
<p>For <span class="math inline">\(t=1,2..., N_{sim}\)</span> :</p>
<p>At iteration <span class="math inline">\(t\)</span>, for <span class="math inline">\(p=1,2..., P\)</span> :</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\theta_1^t \sim p(\theta_1^t | \theta_2^{t-1}, \theta_3^{t-1}, ..., \theta_p^{t-1})\)</span></li>
<li>Generate <span class="math inline">\(\theta_2^t \sim p(\theta_2^t | \theta_1^{t}, \theta_3^{t-1}, ..., \theta_p^{t-1})\)</span></li>
</ol>
<p><span class="math inline">\(\qquad\vdots\)</span></p>
<ol start="16" style="list-style-type: lower-alpha">
<li>Generate <span class="math inline">\(\theta_p^t \sim p(\theta_p^t | \theta_1^{t}, \theta_2^{t}, ..., \theta_{p-1}^{t})\)</span></li>
</ol>
<p>Again, some generic code may provide another way to understand it:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>nsim) {
  <span class="cf">for</span> (p <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>P) {
    thetaNew[p] =<span class="st"> </span><span class="kw">rDistribution</span>(<span class="dv">1</span>, theta[t,<span class="op">-</span>p])
  }
  theta[t,] =<span class="st"> </span>thetaNew
}</code></pre></div>
</div>
<div id="hamiltonian-monte-carlo" class="section level3">
<h3>Hamiltonian Monte Carlo</h3>
<p>Stan uses <span class="emph">Hamiltonian Monte Carlo</span>, another variant of MH. It takes the parameters <span class="math inline">\(\theta\)</span> as collectively denoting the position of a particle in some space with momentum <span class="math inline">\(\phi\)</span> (of same dimension as <span class="math inline">\(\theta\)</span>). Both <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\phi\)</span> are updated at each Metropolis step and jointly estimated, though we are only interested in <span class="math inline">\(\theta\)</span>. We can describe the basic steps as follows.</p>
<ol style="list-style-type: decimal">
<li>At iteration <span class="math inline">\(t\)</span>, take a random draw of momentum <span class="math inline">\(\phi\)</span> from its posterior distribution</li>
<li>Update the position vector <span class="math inline">\(\theta\)</span> given current momentum, update <span class="math inline">\(\phi\)</span> given the gradient of <span class="math inline">\(\theta\)</span></li>
<li>Calculate <span class="math inline">\(r = \frac{p(\theta^*|y)p(\phi^*)}{p(\theta^{t-1})p(\phi^{t-1})}\)</span></li>
<li>Set <span class="math inline">\(\theta^t = \theta^*\)</span> with probability <span class="math inline">\(min(r, 1)\)</span>, else <span class="math inline">\(\theta^t = \theta^{t-1}\)</span></li>
</ol>
<p>The overall process allows it to move quite rapidly through the parameter space, and it can work well where other approaches such as Gibbs might be very slow. An example using HMC on the regression model data can be found in the <a href="appendix.html#hamiltonian-monte-carlo-example">Hamiltonian Monte Carlo Example</a><a href="#fn52" class="footnoteRef" id="fnref52"><sup>52</sup></a>.</p>
</div>
<div id="other-variations-and-approximate-methods" class="section level3">
<h3>Other Variations and Approximate Methods</h3>
<p>Within these MH approaches there are variations such as slice sampling, reversible jump, particle filtering, etc. Also, one can reparameterize the model to help overcome some convergence issues if applicable. In addition, there exist many approximate methods such as Variational Bayes, INLA, Approximate Bayesian Computation, etc. The main thing is just to be familiar with what’s out there in case it might be useful. Any particular method might be particularly well suited to certain models (e.g. INLA for spatial regression models), those that are notably complex, or they may just be convenient for a particular case.</p>
</div>
</div>
<div id="number-of-draws-thinning-warm-up" class="section level2">
<h2>Number of draws, thinning, warm-up</h2>
<p>Whatever program we use, the typical inputs that will need to be set regard the number of simulated draws from the posterior, the number of warm-up draws, and the amount of thinning. Only the draws that remain after warm-up and thinning will be used for inference. However, there certainly is no default that would work from one situation to the next.</p>
<p>Recall that we are looking for convergence to a distribution, and this isn’t determined by the number of draws alone. The fact is that one only needs a few draws for accurate inference. Even something as low as <span class="math inline">\(n_{\textrm{eff}}\)</span> of 10 for each chain would actually be fine assuming everything else seemed in order, though typically we want more than that so that our values don’t bounce around from one model run to the next. To feel confident about convergence, i.e. get <span class="math inline">\(\hat R\)</span> of around 1, plots looking right, etc., we will usually want in the thousands for the number of total draws. We might need quite a few more for increasing model complexity.</p>
<p>A conservative approach to the number of warm-up draws is half the number of runs, but this is fairly arbitrary. Thinning isn’t specifically necessary for inference if approximate convergence is achieved, but is useful with increasing model complexity to reduce autocorrelation among the estimates.</p>
<p>For myself, I typically run models such that the results are based on roughly <span class="math inline">\(n_{\textrm{eff}} = 1000\)</span> estimates, simply because 1000 is a nice round number and is enough to make graphical display a bit smoother. For a regression model as we have been running, the setting we used produced 1000 final estimates across 4 chains. Other models might make due with 100000, 50000, 50 respectively. You may just need to feel things out for yourself, but for simpler models you won’t need much.</p>
</div>
<div id="model-complexity" class="section level2">
<h2>Model Complexity</h2>
<p>One of the great things about the Bayesian approach is its ability to handle extremely complex models involving lots of parameters. In addition, it will often work better (or at all) in simpler settings where the data under consideration are problematic (e.g. collinearity, separation in the logistic regression setting). While it can be quite an undertaking to set things correctly and debug, re-run etc. and generally go through the trial and error process typically associated with highly complex models, it’s definitely nice to know that you can at least attempt them. It will take some work, but you will also learn a great deal along the way. Furthermore, there are typically tips and tricks that can potentially help just about any model run a little more smoothly.</p>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="47">
<li id="fn47"><p>It really should be a part of most analysis.<a href="issues.html#fnref47">↩</a></p></li>
<li id="fn48"><p>With Stan I sometimes do a 1 iteration compile check first.<a href="issues.html#fnref48">↩</a></p></li>
<li id="fn49"><p>The BUGS book has many examples for a wide variety of applications. The <a href="https://github.com/stan-dev/example-models/wiki">Stan github page</a> has Stan examples for each of those BUGS examples and many more.<a href="issues.html#fnref49">↩</a></p></li>
<li id="fn50"><p>Originally developed in physics in the 50s, it eventually made its way across to other fields.<a href="issues.html#fnref50">↩</a></p></li>
<li id="fn51"><p>Given a proposal/jumping distribution <span class="math inline">\(\mathcal{J}_t\)</span>, <br> <span class="math inline">\(r=\frac{p(\theta^*|y)/\mathcal{J}_t(\theta^*|\theta^{t-1})} {p(\theta^{t-1}|y)/\mathcal{J}_t(\theta^{t-1}|\theta^*)}\)</span><a href="issues.html#fnref51">↩</a></p></li>
<li id="fn52"><p>See this entry at David Mimno’s blog for a <a href="http://www.mimno.org/articles/hmc/">visualization of HMC</a>, and Betancourt’s <a href="https://arxiv.org/pdf/1701.02434.pdf">Conceptual Introduction to Hamiltonian Monte Carlo</a>.<a href="issues.html#fnref52">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="model-enhancements.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="final-thoughts.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"],
"epub": true
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["Bayesian-Basics.epub"],
"toc": {
"collapse": "section",
"toc_depth": 3,
"scroll_highlight": true,
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/%s"
}
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
